{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Customer Emotion Analysis System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJQQcRZOazkJ",
        "outputId": "c29f3201-9fd2-4184-b0c4-5d8123e4d2f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Generated Clusters ---\n",
            "Cluster 0: ['pretty', 'easy', 'clean,', 'easily', 'like', 'help', 'lose', 'fit', 'yummy', 'happy']...\n",
            "Cluster 6: ['good', 'good,', 'good.', 'good!', 'Good', 'GOOD', 'good:', '\"Good', '\"good']\n",
            "Cluster 4: ['better', 'better,', 'better!', 'better.', '\"better', 'BETTER.', 'Better']\n",
            "Cluster 3: ['great,', 'great', 'great.', 'Great', 'great!', 'Great!!!', 'GREAT!!', 'GREAT', 'GREAT.', 'Great!']\n",
            "Cluster 2: ['bonus,', 'bonus', 'bonus?']\n",
            "Cluster 7: ['love', 'love,', 'Love', 'LOVE', 'love.', 'Love,']\n",
            "Cluster 5: ['friends,', 'friends.', 'friends', 'friends!', 'Friends']\n",
            "Cluster 1: ['disappointing.', 'disappointing', 'disappointing!', 'disappointing!!!', 'disappointing,']\n",
            "\n",
            "Loading Word2Vec model...\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "\n",
            "--- Cluster Labels ---\n",
            "{0: 'sadness', 6: 'sadness', 4: 'trust', 3: 'joy', 2: 'joy', 7: 'joy', 5: 'joy', 1: 'sadness'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-8bcaa97150ee>:196: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"adorescore\"].fillna(0, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All done! Updated dataset saved to: /content/drive/MyDrive/updated_dataset_auto.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# 1. CONFIGURATION & SETUP\n",
        "\n",
        "\n",
        "# Ensure required packages are installed:\n",
        "# pip install gensim nltk textblob scikit-learn\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download(\"vader_lexicon\", quiet=True)\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "\n",
        "# File paths (adjust as needed)\n",
        "INPUT_CSV = \"/content/drive/MyDrive/dataset.csv\"\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/updated_dataset_auto.csv\"\n",
        "\n",
        "# Number of clusters for word clustering\n",
        "NUM_CLUSTERS = 8\n",
        "\n",
        "# Predefined emotion categories for Word2Vec matching\n",
        "EMOTION_CATEGORIES = {\n",
        "    \"joy\": [\"happy\", \"joyful\", \"excited\", \"cheerful\", \"delighted\"],\n",
        "    \"sadness\": [\"sad\", \"depressed\", \"unhappy\", \"miserable\"],\n",
        "    \"anger\": [\"angry\", \"furious\", \"frustrated\", \"irritated\"],\n",
        "    \"fear\": [\"scared\", \"afraid\", \"terrified\", \"nervous\"],\n",
        "    \"surprise\": [\"surprised\", \"amazed\", \"shocked\"],\n",
        "    \"trust\": [\"trusting\", \"faithful\", \"reliable\", \"hopeful\"],\n",
        "    \"disgust\": [\"disgusted\", \"nauseated\", \"repulsed\"],\n",
        "    \"anticipation\": [\"eager\", \"hopeful\", \"expectant\"],\n",
        "}\n",
        "\n",
        "\n",
        "# 2. LOAD DATA\n",
        "\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "df.columns = df.columns.str.strip()  # Clean column names\n",
        "\n",
        "# Convert rating to numeric and drop rows with missing review or rating\n",
        "df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")\n",
        "df.dropna(subset=[\"review\", \"rating\"], inplace=True)\n",
        "\n",
        "\n",
        "# 3. EXTRACT SENTIMENT WORDS & CLUSTERING\n",
        "\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def extract_sentiment_words(text: str) -> list:\n",
        "    \"\"\"Return words with significant sentiment (|compound| > 0.3).\"\"\"\n",
        "    words = text.split()\n",
        "    return [w for w in words if abs(sia.polarity_scores(w)[\"compound\"]) > 0.3]\n",
        "\n",
        "# Collect all sentiment words from reviews\n",
        "all_words = []\n",
        "df[\"review\"].astype(str).apply(lambda x: all_words.extend(extract_sentiment_words(x)))\n",
        "word_freq = Counter(all_words)\n",
        "emotion_words = list(word_freq.keys())\n",
        "\n",
        "# TF-IDF representation\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(emotion_words)\n",
        "\n",
        "# KMeans clustering of emotion words\n",
        "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Build a mapping from cluster id to list of words\n",
        "cluster_to_words = {}\n",
        "for i, word in enumerate(emotion_words):\n",
        "    cluster = kmeans.labels_[i]\n",
        "    cluster_to_words.setdefault(cluster, []).append(word)\n",
        "\n",
        "print(\"\\n--- Generated Clusters ---\")\n",
        "for cluster, words in cluster_to_words.items():\n",
        "    print(f\"Cluster {cluster}: {words[:10]}{'...' if len(words)>10 else ''}\")\n",
        "\n",
        "\n",
        "# 4. ASSIGN MEANINGFUL EMOTION NAMES USING WORD2VEC\n",
        "\n",
        "\n",
        "print(\"\\nLoading Word2Vec model...\")\n",
        "word_vectors = api.load(\"word2vec-google-news-300\")  # Pre-trained model\n",
        "\n",
        "# Compute mean vector for each predefined emotion\n",
        "emotion_vectors = {}\n",
        "for emotion, synonyms in EMOTION_CATEGORIES.items():\n",
        "    valid_syns = [word_vectors[w] for w in synonyms if w in word_vectors]\n",
        "    if valid_syns:\n",
        "        emotion_vectors[emotion] = np.mean(valid_syns, axis=0)\n",
        "\n",
        "def get_cluster_emotion_label(words: list) -> str:\n",
        "    \"\"\"Assigns a meaningful emotion label to a cluster using cosine similarity.\"\"\"\n",
        "    valid_vecs = [word_vectors[w] for w in words if w in word_vectors]\n",
        "    if not valid_vecs:\n",
        "        return \"neutral\"\n",
        "    cluster_mean = np.mean(valid_vecs, axis=0)\n",
        "    best_emotion, best_score = \"neutral\", -1\n",
        "    for emotion, em_vec in emotion_vectors.items():\n",
        "        score = cosine_similarity([cluster_mean], [em_vec])[0][0]\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_emotion = emotion\n",
        "    return best_emotion\n",
        "\n",
        "# Map each cluster to a meaningful emotion label\n",
        "cluster_labels = {cluster: get_cluster_emotion_label(words) for cluster, words in cluster_to_words.items()}\n",
        "\n",
        "print(\"\\n--- Cluster Labels ---\")\n",
        "print(cluster_labels)\n",
        "\n",
        "\n",
        "# 5. ASSIGN EMOTIONS TO REVIEWS (PRIMARY & SECONDARY)\n",
        "\n",
        "\n",
        "def assign_emotions_to_text(text: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Assign primary and secondary emotions for a given text.\n",
        "    Uses the cluster mapping and also considers overall sentiment\n",
        "    to avoid mismatches (e.g., avoid negative labels if overall review is positive).\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    overall_sentiment = TextBlob(text).sentiment.polarity  # -1 to 1\n",
        "    primary_emotion, secondary_emotion = \"neutral\", \"neutral\"\n",
        "    primary_intensity, secondary_intensity = 0.0, 0.0\n",
        "\n",
        "    for w in words:\n",
        "        # Get the cluster for this word via TF-IDF representation\n",
        "        w_vec = vectorizer.transform([w])\n",
        "        cluster_id = kmeans.predict(w_vec)[0]\n",
        "        emotion_label = cluster_labels.get(cluster_id, \"neutral\")\n",
        "\n",
        "        # Filter based on overall sentiment: if review is positive, skip negative labels\n",
        "        if overall_sentiment > 0.2 and emotion_label in [\"sadness\", \"anger\", \"fear\", \"disgust\"]:\n",
        "            continue\n",
        "        if overall_sentiment < -0.2 and emotion_label in [\"joy\", \"trust\", \"anticipation\", \"surprise\"]:\n",
        "            continue\n",
        "\n",
        "        if primary_emotion == \"neutral\" and emotion_label != \"neutral\":\n",
        "            primary_emotion = emotion_label\n",
        "            primary_intensity = round(np.random.uniform(0.6, 1.0), 2)\n",
        "        elif secondary_emotion == \"neutral\" and emotion_label != \"neutral\" and emotion_label != primary_emotion:\n",
        "            secondary_emotion = emotion_label\n",
        "            secondary_intensity = round(np.random.uniform(0.3, 0.7), 2)\n",
        "\n",
        "    # Fallback: if no emotion was assigned from clusters, use overall sentiment\n",
        "    if primary_emotion == \"neutral\":\n",
        "        if overall_sentiment > 0.2:\n",
        "            primary_emotion = \"joy\"\n",
        "            primary_intensity = round(overall_sentiment, 2)\n",
        "        elif overall_sentiment < -0.2:\n",
        "            primary_emotion = \"sadness\"\n",
        "            primary_intensity = round(abs(overall_sentiment), 2)\n",
        "    return primary_emotion, primary_intensity, secondary_emotion, secondary_intensity\n",
        "\n",
        "# Assign topic from category column (if applicable)\n",
        "df[\"topic\"] = df[\"category\"]\n",
        "\n",
        "# Apply emotion assignment to each review\n",
        "df[\"primary_emotion\"], df[\"primary_intensity\"], df[\"secondary_emotion\"], df[\"secondary_intensity\"] = zip(\n",
        "    *df[\"review\"].astype(str).apply(assign_emotions_to_text)\n",
        ")\n",
        "\n",
        "\n",
        "# 6. SENTIMENT & ADORESCORE CALCULATION\n",
        "\n",
        "\n",
        "def compute_adorescore(rating, sentiment, intensity) -> float:\n",
        "    \"\"\"Compute a weighted score using rating, sentiment polarity, and intensity.\"\"\"\n",
        "    if pd.isna(rating) or pd.isna(sentiment) or pd.isna(intensity):\n",
        "        return np.nan\n",
        "    return round((rating / 5) * 0.4 + (sentiment * 0.3) + (intensity * 0.3), 2)\n",
        "\n",
        "# Overall sentiment using TextBlob\n",
        "df[\"sentiment\"] = df[\"review\"].apply(lambda x: TextBlob(x).sentiment.polarity if x.strip() else 0.0)\n",
        "\n",
        "# Ensure intensities are numeric\n",
        "df[\"primary_intensity\"] = pd.to_numeric(df[\"primary_intensity\"], errors=\"coerce\")\n",
        "\n",
        "# Compute AdoreScore for each review\n",
        "df[\"adorescore\"] = df.apply(\n",
        "    lambda row: compute_adorescore(row[\"rating\"], row[\"sentiment\"], row[\"primary_intensity\"]), axis=1\n",
        ")\n",
        "df[\"adorescore\"].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "# 7. SAVE FINAL OUTPUT\n",
        "\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"\\nAll done! Updated dataset saved to: {OUTPUT_CSV}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "d97404969a224996bcfa99031f59b59f",
            "57ee2aa213614a0a972955b3e8137a60",
            "94697e83baf24568bd301161d9b969da",
            "a7fdfe89badf42b797b8195e8dd28b4a",
            "c893ca6b86f44e2db4637bac7f987c16",
            "8a4a4d6f6f0f491da23fd746b244feb6",
            "e78c417073ce4f208c72c5f09afb36e2",
            "757f75657e61479497a59ee24b50c5a8",
            "b1b4913d9bda4153aba9c93bc84c7a80",
            "514412f18e7b4a7f9dcf82c80136bd61",
            "6e2ef34b33974c1382555430302a4c61"
          ]
        },
        "id": "J-ZfpePsp0Tq",
        "outputId": "13c88914-ac9c-4e88-f3e6-cbaaf25de170"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset with 1494 records.\n",
            "Loading Word2Vec model...\n",
            "Final Cluster Emotion Labels: {0: 'sadness', 6: 'sadness', 4: 'trust', 3: 'joy', 2: 'joy', 7: 'joy', 5: 'joy', 1: 'sadness'}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d97404969a224996bcfa99031f59b59f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for adorescore model: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n",
            "Adorescore Model Performance: MAE=0.08, R²=0.23\n",
            "Adorescore model saved to /content/drive/MyDrive/adorescore_model.pkl\n",
            "Topic classifier saved to /content/drive/MyDrive/topic_classifier.pkl\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# 1. CONFIGURATION & SETUP\n",
        "\n",
        "\n",
        "nltk.download(\"vader_lexicon\", quiet=True)\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "\n",
        "# File paths\n",
        "DATASET_CSV = \"/content/drive/MyDrive/updated_dataset_auto.csv\"\n",
        "ADORESCORE_MODEL_PICKLE = \"/content/drive/MyDrive/adorescore_model.pkl\"\n",
        "TOPIC_MODEL_PICKLE = \"/content/drive/MyDrive/topic_classifier.pkl\"\n",
        "\n",
        "# Number of clusters for emotion word clustering\n",
        "NUM_CLUSTERS = 8\n",
        "\n",
        "# Predefined emotion categories for Word2Vec mapping\n",
        "EMOTION_CATEGORIES = {\n",
        "    \"joy\": [\"happy\", \"joyful\", \"excited\", \"cheerful\", \"delighted\"],\n",
        "    \"sadness\": [\"sad\", \"depressed\", \"unhappy\", \"miserable\"],\n",
        "    \"anger\": [\"angry\", \"furious\", \"frustrated\", \"irritated\"],\n",
        "    \"fear\": [\"scared\", \"afraid\", \"terrified\", \"nervous\"],\n",
        "    \"surprise\": [\"surprised\", \"amazed\", \"shocked\"],\n",
        "    \"trust\": [\"trusting\", \"faithful\", \"reliable\", \"hopeful\"],\n",
        "    \"disgust\": [\"disgusted\", \"nauseated\", \"repulsed\"],\n",
        "    \"anticipation\": [\"eager\", \"hopeful\", \"expectant\"],\n",
        "}\n",
        "\n",
        "# Initialize stopwords, lemmatizer, and VADER sentiment analyzer\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "LEMMATIZER = nltk.WordNetLemmatizer()\n",
        "SIA = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "# 2. HELPER FUNCTIONS\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean text: lowercase, remove URLs/punctuation, tokenize, remove stopwords, and lemmatize.\"\"\"\n",
        "    try:\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r\"http\\S+\", \"\", text)\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "        tokens = word_tokenize(text)\n",
        "        cleaned = [LEMMATIZER.lemmatize(token) for token in tokens if token not in STOPWORDS]\n",
        "        return \" \".join(cleaned)\n",
        "    except Exception as e:\n",
        "        print(f\"Text cleaning error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Emotion Assignment Pipeline\n",
        "\n",
        "\n",
        "def extract_sentiment_words(text: str) -> list:\n",
        "    \"\"\"Extract words with significant sentiment (|compound| > 0.3).\"\"\"\n",
        "    words = text.split()\n",
        "    return [w for w in words if abs(SIA.polarity_scores(w)[\"compound\"]) > 0.3]\n",
        "\n",
        "def build_emotion_clusters(df: pd.DataFrame, review_col: str = \"review\") -> tuple:\n",
        "    \"\"\"\n",
        "    Extract sentiment words from reviews, vectorize using TF-IDF,\n",
        "    cluster them with KMeans, and return the vectorizer, KMeans model, and a cluster mapping.\n",
        "    \"\"\"\n",
        "    all_words = []\n",
        "    df[review_col].astype(str).apply(lambda x: all_words.extend(extract_sentiment_words(x)))\n",
        "    word_freq = Counter(all_words)\n",
        "    emotion_words = list(word_freq.keys())\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(emotion_words)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    cluster_map = {}\n",
        "    for i, word in enumerate(emotion_words):\n",
        "        cluster = kmeans.labels_[i]\n",
        "        cluster_map.setdefault(cluster, []).append(word)\n",
        "    return vectorizer, kmeans, cluster_map\n",
        "\n",
        "def load_word2vec_model():\n",
        "    \"\"\"Load the pre-trained Word2Vec model (Google News 300).\"\"\"\n",
        "    print(\"Loading Word2Vec model...\")\n",
        "    return api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def build_emotion_vectors(wv_model, emotion_categories: dict) -> dict:\n",
        "    \"\"\"For each predefined emotion, compute the mean vector using valid synonyms.\"\"\"\n",
        "    emotion_vecs = {}\n",
        "    for emotion, synonyms in emotion_categories.items():\n",
        "        valid_vecs = [wv_model[w] for w in synonyms if w in wv_model]\n",
        "        if valid_vecs:\n",
        "            emotion_vecs[emotion] = np.mean(valid_vecs, axis=0)\n",
        "    return emotion_vecs\n",
        "\n",
        "def get_cluster_emotion_label(cluster_words: list, wv_model, emotion_vecs: dict) -> str:\n",
        "    \"\"\"\n",
        "    Compute the mean embedding for a cluster and return the emotion\n",
        "    with the highest cosine similarity from the predefined emotion vectors.\n",
        "    \"\"\"\n",
        "    valid_vecs = [wv_model[w] for w in cluster_words if w in wv_model]\n",
        "    if not valid_vecs:\n",
        "        return \"neutral\"\n",
        "    cluster_mean = np.mean(valid_vecs, axis=0)\n",
        "    best_emotion, best_score = \"neutral\", -1\n",
        "    for emotion, em_vec in emotion_vecs.items():\n",
        "        score = cosine_similarity([cluster_mean], [em_vec])[0][0]\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_emotion = emotion\n",
        "    return best_emotion\n",
        "\n",
        "def build_cluster_labels(cluster_map: dict, wv_model, emotion_vecs: dict) -> dict:\n",
        "    \"\"\"\n",
        "    For each cluster in the cluster map, assign a meaningful emotion label\n",
        "    using the Word2Vec-based similarity approach.\n",
        "    \"\"\"\n",
        "    cluster_labels = {}\n",
        "    for cluster, words in cluster_map.items():\n",
        "        label = get_cluster_emotion_label(words, wv_model, emotion_vecs)\n",
        "        cluster_labels[cluster] = label\n",
        "    return cluster_labels\n",
        "\n",
        "\n",
        "# Topic Classifier Functions\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def train_topic_classifier(input_csv: str = DATASET_CSV, model_pickle: str = TOPIC_MODEL_PICKLE) -> dict:\n",
        "    \"\"\"\n",
        "    Train a topic classifier using the cleaned review text to predict the review's category.\n",
        "    Uses TF-IDF vectorization and Logistic Regression.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_csv)\n",
        "    if \"review\" not in df.columns or \"category\" not in df.columns:\n",
        "        raise ValueError(\"CSV must contain 'review' and 'category' columns.\")\n",
        "    df[\"cleaned_review\"] = df[\"review\"].apply(clean_text)\n",
        "    df.dropna(subset=[\"category\"], inplace=True)\n",
        "    X = df[\"cleaned_review\"]\n",
        "    y = df[\"category\"]\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "\n",
        "    vectorizer_topic = TfidfVectorizer(max_features=5000)\n",
        "    X_vec = vectorizer_topic.fit_transform(X)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    clf.fit(X_vec, y_encoded)\n",
        "\n",
        "    topic_model = {\"vectorizer\": vectorizer_topic, \"classifier\": clf, \"label_encoder\": le}\n",
        "    with open(model_pickle, \"wb\") as f:\n",
        "        pickle.dump(topic_model, f)\n",
        "    print(f\"Topic classifier saved to {model_pickle}\")\n",
        "    return topic_model\n",
        "\n",
        "\n",
        "# Adorescore Model Functions\n",
        "\n",
        "\n",
        "def train_adorescore_model(input_csv: str = DATASET_CSV, model_pickle: str = ADORESCORE_MODEL_PICKLE) -> tuple:\n",
        "    \"\"\"\n",
        "    Train an XGBoost regression model (with hyperparameter tuning) using SentenceTransformer embeddings\n",
        "    to predict the adorescore from cleaned reviews.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_csv)\n",
        "    if \"review\" not in df.columns or \"adorescore\" not in df.columns:\n",
        "        raise ValueError(\"CSV must contain 'review' and 'adorescore' columns.\")\n",
        "    df[\"cleaned_review\"] = df[\"review\"].apply(clean_text)\n",
        "    reviews = df[\"cleaned_review\"].tolist()\n",
        "    y = df[\"adorescore\"].values.astype(float)\n",
        "\n",
        "    embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    X_text = embed_model.encode(reviews, show_progress_bar=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "        \"max_depth\": [3, 5]\n",
        "    }\n",
        "    reg = xgb.XGBRegressor(random_state=42)\n",
        "    grid = GridSearchCV(reg, param_grid, cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_model = grid.best_estimator_\n",
        "    print(\"Best parameters for adorescore model:\", grid.best_params_)\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Adorescore Model Performance: MAE={mae:.2f}, R²={r2:.2f}\")\n",
        "\n",
        "    model_data = {\"embedding_model\": embed_model, \"regressor\": best_model}\n",
        "    with open(model_pickle, \"wb\") as f:\n",
        "        pickle.dump(model_data, f)\n",
        "    print(f\"Adorescore model saved to {model_pickle}\")\n",
        "    return best_model, embed_model\n",
        "\n",
        "def main():\n",
        "    # Load dataset\n",
        "    df_updated = pd.read_csv(DATASET_CSV)\n",
        "    print(f\"Loaded dataset with {len(df_updated)} records.\")\n",
        "\n",
        "    # Build emotion clustering pipeline\n",
        "    vectorizer, kmeans, cluster_map = build_emotion_clusters(df_updated, review_col=\"review\")\n",
        "\n",
        "    # Load Word2Vec model and build emotion vectors\n",
        "    wv_model = load_word2vec_model()\n",
        "    emotion_vecs = build_emotion_vectors(wv_model, EMOTION_CATEGORIES)\n",
        "\n",
        "    # Build cluster labels\n",
        "    cluster_labels = build_cluster_labels(cluster_map, wv_model, emotion_vecs)\n",
        "    print(\"Final Cluster Emotion Labels:\", cluster_labels)\n",
        "\n",
        "    # Train adorescore model\n",
        "    train_adorescore_model(input_csv=DATASET_CSV)\n",
        "\n",
        "    # Train topic classifier\n",
        "    train_topic_classifier(input_csv=DATASET_CSV)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QtVHHeqd0y0",
        "outputId": "97e79153-dde6-48a3-d484-13f059a86f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your review: The pizza was absolutely delicious! The crust was perfectly crispy, the cheese was gooey, and the toppings were fresh. I’ll definitely order again!\n",
            "\n",
            "--- Prediction Results ---\n",
            "Predicted AdoreScore: 0.70\n",
            "Overall Sentiment: 0.57\n",
            "Predicted Topic/Category: Grocery & Gourmet Food\n",
            "Prediction saved to final_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download(\"vader_lexicon\", quiet=True)\n",
        "\n",
        "# File paths for the pre-trained models\n",
        "ADORESCORE_MODEL_PICKLE = \"/content/drive/MyDrive/adorescore_model.pkl\"\n",
        "TOPIC_MODEL_PICKLE = \"/content/drive/MyDrive/topic_classifier.pkl\"\n",
        "\n",
        "# Initialize NLTK objects\n",
        "SIA = SentimentIntensityAnalyzer()\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def predict_adorescore(review_text: str, regressor, embed_model) -> float:\n",
        "    cleaned = clean_text(review_text)\n",
        "    X_emb = embed_model.encode([cleaned])\n",
        "    score = regressor.predict(X_emb)[0]\n",
        "    return score\n",
        "\n",
        "def predict_topic(review_text: str, topic_model: dict) -> str:\n",
        "    cleaned = clean_text(review_text)\n",
        "    vec = topic_model[\"vectorizer\"].transform([cleaned])\n",
        "    pred = topic_model[\"classifier\"].predict(vec)[0]\n",
        "    topic_label = topic_model[\"label_encoder\"].inverse_transform([pred])[0]\n",
        "    return topic_label\n",
        "\n",
        "def load_models():\n",
        "    # Load adorescore model (regressor + embedding model)\n",
        "    with open(ADORESCORE_MODEL_PICKLE, \"rb\") as f:\n",
        "        adorescore_data = pickle.load(f)\n",
        "    regressor = adorescore_data[\"regressor\"]\n",
        "    embed_model = adorescore_data[\"embedding_model\"]\n",
        "\n",
        "    # Load topic classifier model\n",
        "    with open(TOPIC_MODEL_PICKLE, \"rb\") as f:\n",
        "        topic_model = pickle.load(f)\n",
        "\n",
        "    return regressor, embed_model, topic_model\n",
        "\n",
        "def main():\n",
        "    regressor, embed_model, topic_model = load_models()\n",
        "    review_input = input(\"Enter your review: \")\n",
        "\n",
        "    # Generate predictions\n",
        "    adorescore = predict_adorescore(review_input, regressor, embed_model)\n",
        "    topic = predict_topic(review_input, topic_model)\n",
        "    overall_sentiment = TextBlob(review_input).sentiment.polarity\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n--- Prediction Results ---\")\n",
        "    print(f\"Predicted AdoreScore: {adorescore:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment:.2f}\")\n",
        "    print(f\"Predicted Topic/Category: {topic}\")\n",
        "\n",
        "    # Optionally, save results to CSV\n",
        "    pred_df = pd.DataFrame([{\n",
        "        \"review\": review_input,\n",
        "        \"predicted_adorescore\": adorescore,\n",
        "        \"overall_sentiment\": overall_sentiment,\n",
        "        \"predicted_topic\": topic\n",
        "    }])\n",
        "    pred_df.to_csv(\"/content/drive/MyDrive/final_predictions.csv\", index=False)\n",
        "    print(\"Prediction saved to final_predictions.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "514412f18e7b4a7f9dcf82c80136bd61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57ee2aa213614a0a972955b3e8137a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a4a4d6f6f0f491da23fd746b244feb6",
            "placeholder": "​",
            "style": "IPY_MODEL_e78c417073ce4f208c72c5f09afb36e2",
            "value": "Batches: 100%"
          }
        },
        "6e2ef34b33974c1382555430302a4c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "757f75657e61479497a59ee24b50c5a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a4a4d6f6f0f491da23fd746b244feb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94697e83baf24568bd301161d9b969da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_757f75657e61479497a59ee24b50c5a8",
            "max": 47,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1b4913d9bda4153aba9c93bc84c7a80",
            "value": 47
          }
        },
        "a7fdfe89badf42b797b8195e8dd28b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_514412f18e7b4a7f9dcf82c80136bd61",
            "placeholder": "​",
            "style": "IPY_MODEL_6e2ef34b33974c1382555430302a4c61",
            "value": " 47/47 [00:07&lt;00:00, 26.53it/s]"
          }
        },
        "b1b4913d9bda4153aba9c93bc84c7a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c893ca6b86f44e2db4637bac7f987c16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d97404969a224996bcfa99031f59b59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57ee2aa213614a0a972955b3e8137a60",
              "IPY_MODEL_94697e83baf24568bd301161d9b969da",
              "IPY_MODEL_a7fdfe89badf42b797b8195e8dd28b4a"
            ],
            "layout": "IPY_MODEL_c893ca6b86f44e2db4637bac7f987c16"
          }
        },
        "e78c417073ce4f208c72c5f09afb36e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
